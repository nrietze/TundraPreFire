#!/bin/bash
#SBATCH --time=0-00:02:00   ## days-hours:minutes:seconds
#SBATCH --cpus-per-task=32   ## ask for 32 cpus (Use greater than 1 for parallelized jobs)
#SBATCH --nodes=1
#SBATCH --output=/logs/job_%j.out
#SBATCH --ntasks=1
#SBATCH --mem-per-cpu=3850
#SBATCH --array=0-19

SLURM_ARRAY_TASK_ID=$1

module load mamba
source activate lpdaac_vitals

echo 'HLS processing starting.'

cd ~/home/nrietz/TundraPreFire/

UTM_TILEID_FILE="code/data_processing/tile.txt"

metrics="["NDVI","NDMI"]"

# Get total number of lines
NUM_LINES=$(wc -l < "$UTM_TILEID_FILE")

# Set number of tasks
NUM_TASKS=20  # Should match the --array size

# Compute chunk size (ceiling division)
CHUNK_SIZE=$(( (NUM_LINES + NUM_TASKS - 1) / NUM_TASKS ))

# Compute start and end lines for this task
START=$(( SLURM_ARRAY_TASK_ID * CHUNK_SIZE + 1 ))
END=$(( START + CHUNK_SIZE - 1 ))

# Extract lines for this task
sed -n "${START},${END}p" "$UTM_TILEID_FILE" > task_${SLURM_ARRAY_TASK_ID}.txt

# Test slurm job
echo python \
    code/data_processing/HLS_preprocessing.py \
    task_${SLURM_ARRAY_TASK_ID}.txt \
    "$metrics"
    --threads=$SLURM_CPUS_PER_TASK \
    --output=/logs/slurm_job_test.out

# Execute HLS preprocessing script with the split UTM tile id file
# python code/data_processing/HLS_preprocessing.py task_${SLURM_ARRAY_TASK_ID}.txt "$metrics"

echo 'finished'
#!/bin/bash -l
#SBATCH --job-name=arrayJob
#SBATCH --time=0-08:00:00   ## days-hours:minutes:seconds
#SBATCH --array=0-19
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=32   ## ask for 32 cpus (Use greater than 1 for parallelized jobs)
#SBATCH --mem-per-cpu=15650
#SBATCH --output=logs/hls_processing_%a.out
#SBATCH --error=logs/hls_processing_%a.err

module load mamba
source activate lpdaac_vitals

cd ~/TundraPreFire/

# Toggle processing steps:
RUN_HLS_DOWNLOAD=1
RUN_HLS_PROCESSING=1
RUN_SEVERITY_SCRIPT=0

if [ $RUN_HLS_DOWNLOAD = 1 ]; then
  # Execute HLS downloading script with the split UTM tile id file
  echo 'HLS download starting.'
  python code/data_processing/HLS_downloading.py tmp/task_${SLURM_ARRAY_TASK_ID}.txt
fi

if [ $RUN_HLS_PROCESSING = 1 ]; then
  # Execute HLS preprocessing script with the split UTM tile id file
  metrics='["NDMI","NDVI","NBR","GEMI"]'

  echo 'HLS preprocessing starting.'
  python code/data_processing/HLS_preprocessing.py tmp/task_${SLURM_ARRAY_TASK_ID}.txt "$metrics"
fi

if [ $RUN_SEVERITY_SCRIPT = 1 ]; then
  echo 'Burn severity calculation starting'
  python code/data_processing/slurm_calculate_burn_severity.py tmp/task_${SLURM_ARRAY_TASK_ID}.txt
fi
echo 'finished'